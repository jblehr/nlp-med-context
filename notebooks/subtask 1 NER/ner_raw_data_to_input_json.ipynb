{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd '/content/drive/Shareddrives/NLP Project'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-a-6y53pIGwf","executionInfo":{"status":"ok","timestamp":1647020149217,"user_tz":360,"elapsed":21241,"user":{"displayName":"Jacob Lehr","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15083895800350701205"}},"outputId":"45d21985-5c90-4722-b1dd-c0f858d72d1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Shareddrives/NLP Project\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zb6TBKdkkWcC"},"outputs":[],"source":["import os\n","from eval_script import RecordTrack1\n","from nltk.tokenize import WordPunctTokenizer\n","import json\n","from nltk.tokenize.punkt import PunktSentenceTokenizer"]},{"cell_type":"code","source":["def process_n2c2_files(base_dir, data_split, tokenizer):\n","    '''\n","    base_dir: (str) base directory (abs or rel to current)\n","    data_split: (str) train, test, or dev\n","    tokenizer: tokenizer instance of nltk library\n","    '''\n","    data_dir = os.path.join(base_dir, data_split)\n","    root_names = [file.replace(\".ann\", \"\") for file in \n","                  os.listdir(data_dir) if \".ann\" in file]\n","\n","    json_output = []\n","    sent_tokenizer = PunktSentenceTokenizer()\n","    for root_name in root_names:\n","        ann_file = os.path.join(data_dir, root_name + \".ann\")\n","        txt_file = os.path.join(data_dir, root_name + \".txt\")\n","        txt = open(txt_file, \"r\")\n","        doc_text = txt.read()\n","        annot_file = open(ann_file, \"r\")\n","        sentences = sent_tokenizer.tokenize(doc_text)\n","        spans = sent_tokenizer.span_tokenize(doc_text)\n","        # Get tokens and spans, where spans are [(start_char, end_char),...]\n","\n","        # Process annotation file for same file\n","        # Only keep medical mentions for now; sort by start pos\n","        annotation = RecordTrack1(ann_file)\n","        all_tags = annotation.annotations['tags'].values()\n","        meds = sorted([tag for tag in all_tags if tag.ttype == \"Drug\"],\n","                       key = lambda item: item.start)\n","        med_idx = 0\n","        \n","        for s_idx, sentence in enumerate(sentences):\n","            tokens = tokenizer.tokenize(sentence)\n","            token_spans = list(tokenizer.span_tokenize(sentence))\n","            labels = [\"O\"] * len(tokens)\n","            sent_start = spans[s_idx][0]\n","\n","            # save character pos in document for later processing\n","            doc_tok_spans = [(start + sent_start, end + sent_start) for (start, end) in token_spans]\n","            \n","            for i, (start_tok, end_tok) in enumerate(token_spans):\n","                if med_idx == len(meds):\n","                    break\n","                if start_tok + sent_start == meds[med_idx].start:\n","                    labels[i] = \"B-MED\"\n","                elif start_tok + sent_start > meds[med_idx].start:\n","                    labels[i] = \"I-MED\"\n","                if end_tok + sent_start >= meds[med_idx].end:\n","                    med_idx += 1\n","\n","            json_output.append({\"tokens\": tokens, \"ner_tags\": labels, \"token_spans\": doc_tok_spans, \"note_id\": root_name})\n","\n","    with open(f\"{base_dir}/input/ner_input/ner_input_{data_split}.json\", 'w') as fp:\n","        fp.write('\\n'.join(json.dumps(i) for i in json_output) +'\\n')\n","    "],"metadata":{"id":"at9PxASsIzwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_dir = 'data/split_data'\n","tokenizer = WordPunctTokenizer()\n","\n","for data_split in ['train', 'dev', 'test']:\n","    process_n2c2_files(base_dir, data_split, tokenizer)"],"metadata":{"id":"sKpaVuB74Wso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ztbQTbuswfEq"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ner_raw_data_to_input_json.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}